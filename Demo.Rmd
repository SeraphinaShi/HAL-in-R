---
title: "Demo"
author: "Seraphina (Junming) Shi"
output:
  html_document:
    toc: true
    toc_float: true
---
<style>
.grey-highlight {
  background-color: #f0f0f0; /* Grey color */
  padding: 2px 4px; /* Adjust padding as needed */
  border-radius: 4px; /* Rounded corners */
}
</style>

```{r setup, include = FALSE} 
knitr::opts_chunk$set(
  autodep=FALSE, warning=FALSE, message=FALSE, echo=TRUE
)
```



Based on the *hal9001* R package by Rachael Phillips, Jeremy Coyle, and Nima Hejazi.


# Objectives
By the end of this demo, you will be able to:  

* Fit a HAL with CV-selected penalty and a undermoothed HAL.  
* Interpret the results from HAL-fit.   
* Estimate the target parameter with HAL as a working model and obtain delta method based confidence intervals.  
  + Average treatment effects.  
  + Conditional average treatment effects.  
  + Dose response curves.  

# 1. Introduction 
The Highly Adaptive Lasso (HAL) is a non-parametric machine learning algorithm that "creates efficient estimators of realistic statistical models of pathwise differential parameters. HAL creates efficient estimators of realistic statistical models of pathwise differential parameters. Furthermore, these models provide asymptotically normal estimators for non-pathwise-differential parameters, which can enable investigators to use HAL-based substitution estimators and the delta method rather than rely on parametric models." [**Highly Adaptive LASSO: Machine Learning That Provides Valid Nonparametric Inference in Realistic Models**] 

This github page is to provide example code for straightforward uses of HAL. 

# 2. How to Fit HAL
In this section, the functions of fitting HAL is demonstrated. 

**Fitting any HAL consists of the following 2 steps**:  
1. Define the outcome vector, Y, and the covariates matrix X.  
2. Fit the HAL with selected smoothness order and other parameters.    

**Running example with WASH Benefits dataset**:  
We will use the WASH Benefits Bangladesh study as an example to guide this demo of HAL-based plug-in estimator. 

#### Preliminaries
First, we need to load the data, relevant functions, and relevant packages into the R session, and then make sure there is no missing values in the dataset. 


**Load the data**
```{r}
library("data.table")

washb_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/tlverse-data/master/",
    "wash-benefits/washb_data.csv"
  ),
  stringsAsFactors = TRUE
)
```

Take a look at the first few rows of WASH dataset:
```{r}
head(washb_data)
```

**Remove missing values**
Here we simply keep the complete cases by removing the rows with missing values. You can choose any other methods to deal with missing values.
```{r}
washb_data <- washb_data[complete.cases(washb_data), ]
```


With the goal of demonstrating how to use HAL, we will only use observations that had the control or WSH interventions. Then we will set the treatment variable (<span class="grey-highlight">tr</span>) to be 1 if being treated by WSH interventions. 
```{r}
washb_data_demo <- washb_data[washb_data$tr %in% c("Control", "WSH"), ]
washb_data_demo$tr = as.numeric(washb_data_demo$tr == "WSH")
table(washb_data_demo$tr)
```

#### Install hal9001 pacakge (as needed)
To install any packge in R, we recommend clearing the R workspace and subsequently restarting the R session. In RStudio, you can accomplish this by navigating to the "Session" tab, selecting "Clear Workspace," and then proceeding to click on "Session" again and choosing "Restart R."

For standard use, we recommend installing the package from CRAN via:

```{r, eval = FALSE}
install.packages("hal9001")
```

#### Load hal9001 package 
Once the <span class="grey-highlight">hal9001</span> package is installed, we can load it and other R packages:
```{r, message=FALSE, warning=FALSE}
library(hal9001)
```

load the <span class="dplyr-highlight">dplyr</span> package for easy and quick data manipulation.    
load the <span class="dplyr-highlight">glmnet</span> package used in the undersmoothing HAL process. 
```{r}
library(dplyr)
library(glmnet)
```

#### Source relevant functions
You need to download the *'functions_UHAL_deltaCI.R'* for the github repo. Then source it to load relevant functions for fitting Undermoothed HAL and obtainning confidence intervals using the delta method.
```{r}
source('functions_UHAL_deltaCI.R')
```

## 2.1 Prepare data

First, identify the outcome variable and save it as a numeric vector. In this example, we are interested in weight-for-height z-score (<span class="dplyr-highlight">whz</span>) as the outcome, which is continuouts. 
```{r}
Y <- as.numeric(washb_data_demo$whz)
head(Y)
```

Then, identify the treatment variable and other covariates, save them as a numeric data frame or a numeric matrix. If the treatment or other variables are categorical or ordinal, we need to convert them into numeric or dummy variables.   
For the WASH dataset, there are many choices of covariate combinations. Since the goal is to demonstrate the use of HAL, we will pick a smaller set of covariates with shorter run times. 

```{r}
# x_names = names(washb_data_demo)[names(washb_data_demo) != 'whz']
washb_data_demo$sexF = as.numeric(washb_data_demo$sex == "female")
x_names = c("tr", "month", "aged", "sexF", "momage", "momheight", "hfiacat", "Nlt18", "Ncomp")
X <- washb_data_demo %>% 
    select(all_of(x_names)) %>% 
    mutate_if(sapply(., is.factor), as.numeric)
head(X)
```


## 2.2 Fit HAL
### 2.2.1 HAL (CV-HAL) 

To fit the CV-HAL using the prepared X and Y above, we call the <span class="dplyr-highlight">fit_hal</span>' function from the <span class="dplyr-highlight">fit_hal</span> package. The following provided  employs the default configuration to train HAL. This involves utilizing 10-fold cross-validation to select lambda, setting <span class="dplyr-highlight">max_degree</span> to 1, <span class="dplyr-highlight">smoothness_orders</span> to 1, <span class="dplyr-highlight">family</span> to "guassian", and incorporating additional arguments like <span class="dplyr-highlight">num_knots</span>. You can look for <span class="dplyr-highlight">fit_hal</span> in the help pane in Rstudio or go to *https://rdrr.io/cran/hal9001/man/fit_hal.html* for more information. To have reproducible results, we will set a random number generator.
```{r}
set.seed(123)

hal_fit <- fit_hal(X = X, Y = Y)

hal_fit$times
```

**Specifying smoothness of the HAL model**    
We can use <span class="dplyr-highlight">smoothness_orders</span> arguement to enforce smoothness on the functional form of the HAL fit.   
* <span class="dplyr-highlight">smoothness_orders = 0</span> provides a segmented constant approximation (via zero-order basis functions) that accommodates abrupt changes within the function. This is valuable when desiring to avoid assuming any level of smoothness or continuity in the underlying function.  
* <span class="dplyr-highlight">smoothness_orders = 1</span> gives a piece-wise linear fit (via first-order basis functions), which is continuous and mostly differentiable.  
* <span class="dplyr-highlight">smoothness_orders = k</span> gives a segmented polynomial fit (via k-order basis functions). 

Mathematically, HAL fitting will find the optimal fit under the constraint that the total variation of the functionâ€™s k-th derivative is bounded by some constant, which is selected through cross-validation.

When not sure about <span class="dplyr-highlight">smoothness_orders</span>, we suggest use the default value 1. 
```{r, eval = FALSE}
hal_fit_zeroorder <- fit_hal(X = X, Y = Y, smoothness_orders = 0)
hal_fit_firstorder <- fit_hal(X = X, Y = Y) d
```


### 2.2.2 Undersmoothed HAL
Three steps to fit a undersmoothed HAL:  
1. We need to fit a cv-hal with the arguement <span class="dplyr-highlight">return_x_basis = TRUE</span> by calling the <span class="dplyr-highlight">fit_hal</span> function.  
2. We pass in <span class="dplyr-highlight">X</span>, <span class="dplyr-highlight">Y</span>, <span class="dplyr-highlight">hal_cv</span> fit object, and <span class="dplyr-highlight">family</span> to the <span class="dplyr-highlight">undersmooth_hal</span> function (from the *'functions_UHAL_deltaCI.R'* file) to get the undersmoothed lambda.   
3. Then we fit the undersmoothed HAL by calling <span class="dplyr-highlight">fit_hal</span> function again with:  
  - <span class="dplyr-highlight">lambda</span> to be the undersmoothed lambda,    
  - <span class="dplyr-highlight">fit_control = list(cv_select = FALSE)</span> indicating no need to select a lambda through cross validation,   
  - and other arguements the same as the initial fitting of cv-hal.   

```{r}
set.seed(123)
hal_cv_fit <- fit_hal(X = X, Y = Y, return_x_basis = TRUE)

undersmooth_hal_results <- undersmooth_hal(X, Y, 
                                           fit_init = hal_cv_fit,
                                           family = "gaussian")

hal_u_fit <- fit_hal(X = X, Y = Y, 
                return_x_basis = TRUE,
                fit_control = list(cv_select = FALSE),
                lambda = undersmooth_hal_results$lambda_under)

```

There can be situations that there is no need for undersmoothing or we cannot undersmooth further. We can check this by whether <span class="dplyr-highlight">lambda_init</span> and <span class="dplyr-highlight">lambda_under</span> in the results of calling <span class="dplyr-highlight">undersmooth_hal</span> function are the same. If this is the case, we can just proceed with <span class="dplyr-highlight">hal_cv</span>. 

## 2.3 Sumarizing HAL fits

The summary of a HAL fit object will have the table of non-zero coefficients and the corresponding basis functions. The lambda used will also be stored here.

```{r}
hal_fit_summary <- summary(hal_u_fit)
print(hal_fit_summary)
```



# 3. Obtainning Predictions 
## 3.1 Predictions

We will use the previourly obtained fitted undersmoothed HAL object, <span class="dplyr-highlight">hal_u_fit</span>, to get the predicted whz value for each individual within the dataset.
```{r}
hal_u_preds <- predict(hal_u_fit, new_data = X)
head(hal_u_preds)
```

### 3.2 Counterfactual predictions
Counterfactual predictions are predicted values under an intervention of interest.   
To continue the example on the WASH Benefits Bangladesh study and using the same subset of the data, suppose we would like to obtain predictions for every subjectâ€™s weight-for-height z-score (<span class="dplyr-highlight">whz</span>) outcome under an treatment (<span class="dplyr-highlight">tr</span>) intervention that sets it to the WSH interventions. To do so, we need to do two simple steps: 1) create a copy of the <span class="dplyr-highlight">X</span> matrix and intervene on <span class="dplyr-highlight">tr</span> in the copied matrix; 2) obtain prediction using the intervened matrix. 

```{r}
# 1. copy data & intervene
X_intervene <- X
X_intervene$tr = 1 
head(X_intervene)
``` 
```{r}
# 2. predictions
counterfactual_pred <- predict(hal_u_fit, new_data = X_intervene)
head(counterfactual_pred)
``` 

# 4. Obtainning Estimates and Inferences 

## 4.1 Counterfactual Outcome Mean
Counterfactual outcome mean is the average outcome that all individuals in the target population would have experienced if they had received a particular treatment or exposure value. 
Suppose we are interested in the average weight-for-height z-score (<span class="dplyr-highlight">whz</span>) if all of the people in Bangladesh had the WSH interventions (i.e. the conterfactual outcome mean): $\mathbb{E}_P(Y_1)$.

```{r}
CMean_infr = CounterfactualMean_inferece(X, Y, treatment = "tr", treatment_level = 1, hal_fit = hal_u_fit, family = "gaussian")
print(CMean_infr)
```

## 4.2 ATE

The average treatment effect (ATE) is a measure of difference in outcomes of interest between the treatment intervention to the control.   
Now suppose we are interested in the causal effect of weight-for-height z-score (<span class="dplyr-highlight">whz</span>) due to WSH interventions, which is the difference between average counterfactual weight-for-height z-score if all of the people in Bangladesh got the WSH interventions versus all of the people in Bangladesh did not have WSH interventions (i.e. the average treatment effect): $$\Psi(P) = \mathbb{E}_P(Y_1) - \mathbb{E}_P(Y_0)$$  where $Y_a$ denotes the counterfactual outcome (weight-for-height z-score), if possibly contrary to fact, the person had interventions status A = a.

```{r}
# Using the provided function to obtain the ATE: 
ATE_infr = ATE_inferece(X, Y, treatment = "tr", hal_fit = hal_u_fit, family = "gaussian")

print(ATE_infr)
```

## 4.2 CATE
The conditional average treatment effect (CATE) is the average treatment effect on an outcome variable for a specific subgroup or conditional population defined by a set of covariates or characteristics.

Suppose we are interested in the causal effect of weight-for-height z-score (<span class="dplyr-highlight">whz</span>) due to WSH interventions among males (i.e. the conditional average treatment effect among males): $$\Psi(P_{male}) = \mathbb{E}_{P_{male}}(Y_1) - \mathbb{E}_{P_{male}}(Y_0)$$.

```{r}
CATE_infr = ATE_inferece(X, Y, treatment = "tr", condition = "sexF", condition_level = 1, hal_fit = hal_u_fit, family = "gaussian")

print(CATE_infr)
```


# 5. Dore Response Curve Example with a Simulated Dataset
## 5.1 Fit HAL
## 5.2 Dose response curve estimatation
## 5.3 Confidence intervals

